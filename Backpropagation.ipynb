{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is the process by which a neural network learns.  The goals of this learning process is to tweak a neural network so that it outputs the correct answer for a given input.  Let's think about how a child learns to recognize objects, such as dogs.  When a child sees a dog for the first time, the child doesn't know what it is and might say it's a big cat.  To teach the child, a teacher might give the child pictures of animals and ask him to pick which ones are dogs.  The teacher will also tell the child that the answers are on the back of each picture so he can check his work.  The child would look through the pictures, guessing which ones were dogs and then comparing that guess with the answer on the back.  The child would quickly learn to identify dogs.  Neural networks can learn in a similar fashion, called supervised learning.  Neural networks use training data, much like how the child used animal pictures with the answers, to learn via backpropagation.\n",
    "\n",
    "Training data is input and output pairs $(x,y)$, where $x$ is the input and $y$ is the output.  $y$ is usually called the target.  The goal is to take the input $x$, pass it through the neural network (feedforward process) and output $\\hat{y}$, where $\\hat{y} \\approx y$.  The problem is, if the parameters of the neural network aren't configured (trained) properly, then the neural network won't output the correct value, meaning $\\hat{y} \\neq y$.  Backpropagation is the process by which the parameters of the neural network are adjusted, so that when passing $x$ through the neural network, it will produce $\\hat{y}$, where $\\hat{y} \\approx y$.  More concretely, if the training example is $(dog picture, yes)$, then when passing a dog picture through a neural network it should output yes.  If the neural network is not trained, then most likely it will output no when indeed the picture is of a dog. The neural network would need to be adjusted through backpropagation so that the neural network would output the correct value, yes.  To learn this process, we will look at a very simple neural network and build on it.  Below is the network we'll be using.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"Images/simple_neural_network_1.png\" style=\"width:50%\" />\n",
    "  <figcaption><b>figure:&nbsp;&nbsp;</b>Simpe Neural Network - 4 Neurons</figcaption>\n",
    "</figure> \n",
    "\n",
    "In the figure above there's 1 input (green), 2 hidden (orange), and 1 output (blue) layer\n",
    "\n",
    "> $a_n$ represents the activation function of the neuron  \n",
    "$w_n$ represents the weights of the network  \n",
    "$\\hat{y}$ is the value of the output node  \n",
    "\n",
    "The way backpropagation works, is to reduce the error/cost in the neural network as efficiently as possible.  This is done by computing the error/cost of the output and adjusting the weights and biases in the network going backwards through each layer.  So the cost of the above neural network would be dependent on all the weights and biases in the network which can be represented like $Cost = C(w_1, b_1, w_2, b_2, w_3, a_3)$.  This neural network is even too big to start describing the backpropagation process.  Let's focus on the last two neurons and their connection,\n",
    "\n",
    "<figure>\n",
    "  <img src=\"Images/simple_neural_network_2.png\" style=\"width:25%\" />\n",
    "  <figcaption><b>figure:&nbsp;&nbsp;</b>Simpe Neural Network - 1 Neuron</figcaption>\n",
    "</figure> \n",
    "\n",
    "The neurons are labeled $a^{L-1}$ and $a^{L}$, where $L$ is an index for a layer.  So since we have a 3 layer network, $L = 3$  \n",
    "Also note that $a^{(L)}$ and $\\hat{y}$ are the exact same thing.  Going forward, until otherwised noted, I will not use the $\\hat{y}$ notation, but only $a^{(L)}$ notation.  It will make the formulas easier to read and understand.\n",
    "\n",
    "During the feedforward process, a training example is passed forward through the network and produces an output, $a^{(L)}$.  In other words, $a^{(L)}$ is the neural networks guess.  So the first step in the backpropagation process is to compute the cost by computing the difference of the guess, $a^{(L)}$, from the target, $y$.  The formula to compute the cost is as follows (note that the formulas are color coded so they can be easily tracked),\n",
    "\n",
    "> $\\color{red}{ C_0 = (\\color{green}{ a^{(L)} } - y)^{2} }$\n",
    "\n",
    "$C_0$ is the cost of the first training example, where $C$ is for Cost and subscript 0 is for first training example.\n",
    "\n",
    "Remember that,\n",
    "> $\\color{green}{ a^{(L)} } = \\color{green}{ \\sigma( } \\color{blue}{ w^{(L)}a^{(L-1)}+b^{(L)} } \\color{green}{ ) }$\n",
    "\n",
    "It's better to think about $a^{L}$ as the following, so that the math and programming work out better,\n",
    "\n",
    "> $\\color{blue}{ z^{(L)} = w^{(L)}a^{(L-1)}+b^{(L)} }$  \n",
    "> $\\color{green}{ a^{(L)} = \\sigma(z^{(L)}) }$\n",
    "\n",
    "These equations can be easily visualized using a computational graph like the one below,\n",
    "\n",
    "<figure>\n",
    "  <img src=\"Images/computation_graph_derivative_1.png\" style=\"width:25%\" />\n",
    "  <figcaption><b>figure:&nbsp;&nbsp;</b>Computation Graph</figcaption>\n",
    "</figure> \n",
    "\n",
    "As you can see from this graph, each function is composed of 1 or more functions or parameters.  For instance, $\\color{red}{ C_0 }$ is composed of $\\color{green}{ a^{(L)} }$ and $\\color{red}{ y }$.  $\\color{green}{ a^{(L)} }$ is composed of $\\color{blue}{ z^{(L)} }$, which is composed of $\\color{blue}{ w^{(L)} }$, $\\color{blue}{ a^{(L-1)} }$, and $\\color{blue}{ b^{(L)} }$.\n",
    "\n",
    "Another way to look at the 3 equations is when it's all combined.\n",
    "\n",
    "$\\color{red}{ C_0 = (\\color{green}{ \\sigma(\\color{blue}{ w^{(L)}a^{(L-1)}+b^{(L)} }) } - y)^{2} }$\n",
    "\n",
    "As a reminder, the goal is to reduce the cost $C_0$ by changing the weights, i.e. $w^{(L)}$.  While looking at the computational graph and/or equations, the question is, \"How is the cost $C_0$ affected by small changes in the weight $w^{(L)}$?\".  For anyone who studied higher level mathematics, this will quickly remind you of Calculus.   Looking at how small changes of parameters affect the function can be done with Calculus using derivatives, and in this case more specifically partial derivatives.  A small change in $w^{(L)}$ is represented as $\\partial w^{(L)}$ and a small change in $C_0$ is represented as $\\partial C_0$.  The $\\partial$ notation can be read as \"del\".  A better way of asking the question is, \"what is the partial derivative of $C_0$ with respect to $w^{(L)}$\" and it can be represented as a ratio like so,\n",
    "\n",
    "> $\\dfrac{\\partial C_0}{\\partial w^{(L)}}$\n",
    "Note: This is read \"The partial derivative of C subcript zero with respect to W L\"\n",
    "\n",
    "So, if our cost function is define like so, \n",
    "\n",
    "$\\color{red}{ C_0 = (\\color{green}{ \\sigma(\\color{blue}{ w^{(L)}a^{(L-1)}+b^{(L)} }) } - y)^{2} }$\n",
    "\n",
    "How is the cost $C_0$ affected by small changes in the weight $w^{(L)}$?  \n",
    "More formally, what is the partial derivative of $C_0$ with respect to $w^{(L)}$? \n",
    "\n",
    "$\\dfrac{\\partial C_0}{\\partial w^{(L)}} = \\text{ ? }$\n",
    "\n",
    "There's a technique to differentiate this function, but we will talk about that a little later.  For now, let's look at it intuitively to gain a better understanding of what's going on.  A small change to $\\color{blue}{ w^{(L)} }$ causes a small change to $\\color{blue}{ z^{(L)} }$ which causes a change to $\\color{green}{ a^{(L)} }$ which causes a change to $\\color{red}{ C_0 }$.\n",
    "\n",
    "Let's break this down and look at it in small chunks. \n",
    "\n",
    "First we have to determine how small changes to $\\color{blue}{ w^{(L)} }$ affect $\\color{blue}{ z^{(L)} }$.  As mentioned before, this question is the same as asking more formally, what is the partial derivative of $\\color{blue}{ z^{(L)} }$ with respect to $\\color{blue}{ w^{(L)} }$?  We can write the first term like so,\n",
    "\n",
    "> $\\dfrac{\\partial C_0}{\\partial w^{(L)}} = \\color{magenta}{ \\dfrac{\\partial z^{(L)}}{\\partial w^{(L)}} }$.\n",
    "\n",
    "Now that we defined how small changes to $\\color{blue}{ w^{(L)} }$ affects $\\color{blue}{ z^{(L)} }$, we need to determine how small changes to $\\color{blue}{ z^{(L)} }$ affect $\\color{green}{ a^{(L)} }$.  We define it using partial derivatives like we did for the term above,\n",
    "\n",
    "> $\\dfrac{\\partial C_0}{\\partial w^{(L)}} = \\color{magenta}{ \\dfrac{\\partial z^{(L)}}{\\partial w^{(L)}} } \\color{orange}{ \\dfrac{\\partial a^{(L)}}{\\partial z^{(L)}} }$.\n",
    "\n",
    "Finally, we need to define how a small change to $\\color{green}{ a^{(L)} }$ affects $\\color{teal}{ C_0 }$,\n",
    "\n",
    "> $\\dfrac{\\partial C_0}{\\partial w^{(L)}} = \\color{magenta}{ \\dfrac{\\partial z^{(L)}}{\\partial w^{(L)}} } \\color{orange}{ \\dfrac{\\partial a^{(L)}}{\\partial z^{(L)}} } \\color{teal}{ \\dfrac{\\partial C_0}{\\partial a^{(L)}} }$.\n",
    "\n",
    "This final equation that we have built up term by term answers the question, \"What is the partial derivative of $C_0$ with respect to $w^{(L)}$?\"  As mentioned earlier, there is a technique to differentiate this function and if you are familiar with Calculus you'll be familiar with this Rule.  The [Chain Rule](https://en.wikipedia.org/wiki/Chain_rule) describes how to compute the derivative of a function of functions or a composite function, i.e.) $f(g(x))$. There are a lot of articles on the Chain Rule, so I won't be going into the details here, but basically it says,\n",
    "\n",
    "$\\dfrac{d}{dx} f(g(x)) = \\dfrac{d}{dx} f(g(x)) \\dfrac{d}{dx} g(x) $\n",
    "\n",
    "or better notation is to use the prime notation $^{\\prime}$ to represent derivatives.  So to make the formula more readable, you'll often see it like the following,\n",
    "\n",
    "$f^{\\prime}(g(x)) = f^{\\prime}(g(x)) g^{\\prime}(x) $\n",
    "\n",
    "If you look at how the forumla for the function $\\dfrac{\\partial C_0}{\\partial w^{(L)}}$ above, you'll see that it's just the chain rule. (Hint: Reverse the terms and it becomes very clear)\n",
    "\n",
    "Now that we have the formula for the $\\dfrac{\\partial C_0}{\\partial w^{(L)}}$, let's now compute the derivatives for each term.  There're a lot of symbols on the page, so let's re-group and lessen the confusion. I listed the computed formulas so far.\n",
    "\n",
    "$\\color{red}{ C_0 = (a^{(L)} - y)^{2} }$  \n",
    "\n",
    "$\\color{blue}{ z^{(L)} = w^{(L)}a^{(L-1)}+b^{(L)} }$    \n",
    "\n",
    "$\\color{green}{ a^{(L)} = \\sigma(z^{(L)}) }$\n",
    "\n",
    "$\\dfrac{\\partial C_0}{\\partial w^{(L)}} = \\color{magenta}{ \\dfrac{\\partial z^{(L)}}{\\partial w^{(L)}} } \\color{orange}{ \\dfrac{\\partial a^{(L)}}{\\partial z^{(L)}} } \\color{teal}{ \\dfrac{\\partial C_0}{\\partial a^{(L)}} }$\n",
    "\n",
    "Now let's compute the derivative of the last formula starting from the last term.  The functions we'll be deriving are the first 3 above (<span style=\"color:red\">red</span>,<span style=\"color:blue\"> blue</span>,<span style=\"color:green\"> green</span>).  I'm not going to go through differentiating each of the 3 functions step-by-step, so I'll just provide them.\n",
    "\n",
    "$\\color{teal}{ \\dfrac{\\partial C_0}{\\partial a^{(L)}} } = 2(a^{(L)} - 1)$  \n",
    "$\\color{orange}{ \\dfrac{\\partial a^{(L)}}{\\partial z^{(L)}} } = \\sigma^{\\prime}(z^{(L)})$  \n",
    "$\\color{magenta}{ \\dfrac{\\partial z^{(L)}}{\\partial w^{(L)}} } = a^{(L-1)}$  \n",
    "\n",
    "Therefore,\n",
    "\n",
    "$\\dfrac{\\partial C_0}{\\partial w^{(L)}} = \\color{magenta}{ \\dfrac{\\partial z^{(L)}}{\\partial w^{(L)}} } \\color{orange}{ \\dfrac{\\partial a^{(L)}}{\\partial z^{(L)}} } \\color{teal}{ \\dfrac{\\partial C_0}{\\partial a^{(L)}} } = a^{(L-1)}  \\sigma^{\\prime}(z^{(L)})  2(a^{(L)} - 1)$\n",
    "\n",
    "Finally, we have formula to compute the cost of one training example on a 1-layer neural network.  We have nearly everything we need to compute the cost for small changes in $w^{L}$, but remember this was only for one training example.  This is why we represented the cost function as $C_0$ where the subscript 0 means first training example.  The total cost is the average cost of all computed costs in the whole training set.  So the formula to compute that is as follows\n",
    "\n",
    "$\\dfrac{\\partial C}{\\partial w^{(L)}}  = \\dfrac{1}{n} \\sum\\limits_{k=0}^{n-1} \\dfrac{\\partial C_k}{\\partial w^{(L)}}$\n",
    "\n",
    "Let's break this down.  $n$ is the number of training examples.  This formula is summing up all the costs for each training example like so, $\\sum\\limits_{k=0}^{n-1} \\dfrac{\\partial C_k}{\\partial w^{(L)}}$ and then divides by the number of training examples using this term, $\\dfrac{1}{n}$.  It's an average of all costs over the entire training set.\n",
    "\n",
    "Looking back at the computational graph above, these calculations were only for the $w^{(L)}$ parameter.  Luckily there's not much difference with the other parameters.  For the last two paramters you'll follow the same process.  You'll end up with the 3 following equations below. \n",
    "\n",
    "$\\dfrac{\\partial C_0}{\\partial w^{(L)}} = \\dfrac{\\partial z^{(L)}}{\\partial w^{(L)}} \\dfrac{\\partial a^{(L)}}{\\partial z^{(L)}} \\dfrac{\\partial C_0}{\\partial a^{(L)}} = a^{(L-1)}  \\sigma^{\\prime}(z^{(L)})  2(a^{(L)} - 1)$  \n",
    "$\\dfrac{\\partial C_0}{\\partial b^{(L)}} = \\dfrac{\\partial z^{(L)}}{\\partial b^{(L)}} \\dfrac{\\partial a^{(L)}}{\\partial z^{(L)}} \\dfrac{\\partial C_0}{\\partial a^{(L)}} = 1  \\sigma^{\\prime}(z^{(L)})  2(a^{(L)} - 1)$  \n",
    "$\\dfrac{\\partial C_0}{\\partial a^{(L-1)}} = \\dfrac{\\partial z^{(L)}}{\\partial a^{(L-1)}} \\dfrac{\\partial a^{(L)}}{\\partial z^{(L)}} \\dfrac{\\partial C_0}{\\partial a^{(L)}} = w^{(L)}  \\sigma^{\\prime}(z^{(L)})  2(a^{(L)} - 1)$  \n",
    "\n",
    "**Note 1:**\n",
    "Sometime you'll see the cost function multiplied by a $\\dfrac{1}{2}$.  This is to cancel out the exponent during differentiation. \n",
    "\n",
    "Original: $C_0 = (a^{(L)} - y)^{2}$  \n",
    "New: $C_0 = \\dfrac{1}{2}(a^{(L)} - y)^{2}$  \n",
    "\n",
    "When the new cost function is differentiated, the 2 cancels out.  This just helps to simplify the equation  \n",
    "\n",
    "Originial: $\\dfrac{\\partial C_0}{\\partial a^{(L)}} = 2(a^{(L)} - 1)$  \n",
    "New: $\\dfrac{\\partial C_0}{\\partial a^{(L)}} = (a^{(L)} - 1)$  \n",
    "\n",
    "**Note 2:**\n",
    "Another thing to keep in mind is sometimes you'll see the cost function written two ways\n",
    "\n",
    "Original: $C_0 = (a^{(L)} - y)^{2}$  \n",
    "New: $C_0 = (y - a^{(L)})^{2}$  \n",
    "\n",
    "This is fine, but when you update the neural network parameters you will either add or subtract\n",
    "\n",
    "Original: add the update  \n",
    "New: subtract the update  \n",
    "\n",
    "That's basically it.  I'll write more on how this applies to a more complex network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
