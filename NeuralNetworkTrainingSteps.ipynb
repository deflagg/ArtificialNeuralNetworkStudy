{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Training Process: Feedforward & Backpropagation Step-By-Step "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "\n",
    "Let's start by introducing some notation for how we will describe layers, nodes, and weights.\n",
    " - $a\\implies$ Neuron/Activation\n",
    " - $a_2\\implies$ bias units\n",
    " - $L\\implies$ Layer\n",
    " - $w\\implies$ Weight\n",
    " - $jk\\implies$ Weight from node j to node k (right to left)\n",
    " - $y_0\\implies$ First output\n",
    " - $y_1\\implies$ Second output\n",
    "\n",
    "### Neural Network Topology\n",
    "\n",
    "Below is a 3-layer network topology that we'll be using as a example for the walkthrough.  The top image shows how the notation is defined, and the computational graph is shown below the horizontal line.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"Images/simple_neural_network_3.png\" style=\"width:80%;\" />\n",
    "</figure> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Dataset\n",
    "The first thing to do is import a dataset.  We'll be using a test dataset for this walkthrough with the following properties\n",
    "\n",
    "- **Legs**\n",
    "- **Language**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Legs</th>\n",
       "      <th>Language</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Animal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Legs  Language  Target\n",
       "0     2         1   Human\n",
       "1     4         0  Animal"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"Datasets/test.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "Let's initialize the neural network by setting the weights randomly.  In the image below you can see the random weights between the range 0 and 1 (exclusive), the bias nodes are set to $+1$. Read [Neural Network Initialization](NeuralNetworkInitialization.ipynb) to learn more about different methods to initialize a neural network. The output of the neural network will be defined as follows.  \n",
    "\n",
    "$\\begin{bmatrix}a^{(3)}_{0} \\\\ a^{(3)}_{1} \\end{bmatrix} = \\begin{bmatrix}0 \\\\ 1 \\end{bmatrix} = \\rm{Human}$  \n",
    "$\\begin{bmatrix}a^{(3)}_{0} \\\\ a^{(3)}_{1} \\end{bmatrix} = \\begin{bmatrix}1 \\\\ 0 \\end{bmatrix} = \\rm{Animal}$  \n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"Images/simple_neural_network_4.png\" style=\"width:60%\" />\n",
    "</figure> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward\n",
    "\n",
    "##### Set the inputs, $a^{(0)}_{0}$ and $a^{(0)}_{1}$\n",
    "$a^{(0)}_{0} = 2$  \n",
    "$a^{(0)}_{1} = 1$  \n",
    "Layers $a^{(1)}$ and $a^{(2)}$ will use the sigmoid activation function $= g(x) = \\dfrac{1}{1+e^{-x}}$  \n",
    "Layer $a^{(3)}$ will use the softmax activation function $= \\sigma(z)_j = \\dfrac{e^{z_j}}{\\sum_{k=1}^{K}e^{(z_k)}}\\quad \\textrm{for } j=1,..., K$  \n",
    "\n",
    "##### Compute $a^{(1)}_{0}$\n",
    "$z^{(1)}_0 = w^{(1)}_{0,0} \\cdot a^{(0)}_{0} + w^{(1)}_{0,1} \\cdot a^{(0)}_{1} + w^{(1)}_{0,2} \\cdot a^{(0)}_{2} = 0.63 \\cdot 2 + 0.39 \\cdot 1 + 0.34 \\cdot 1 = 1.99$  \n",
    "$a^{(1)}_{0} = g(z^{(1)}_0) = \\dfrac{1}{1+e^{-z^{(1)}_0}} = \\dfrac{1}{1+e^{-1.99}} \\approx 0.88$  \n",
    "\n",
    "##### Compute $a^{(1)}_{1}$\n",
    "$z^{(1)}_1 = w^{(1)}_{1,0} \\cdot a^{(0)}_{0} + w^{(1)}_{1,1} \\cdot a^{(0)}_{1} + w^{(1)}_{1,2} \\cdot a^{(0)}_{2} = 0.50 \\cdot 2 + 0.50 \\cdot 1 + 0.71 \\cdot 1 = 2.21$  \n",
    "$a^{(1)}_{1} = g(z^{(1)}_1) = \\dfrac{1}{1+e^{-z^{(1)}_1}} = \\dfrac{1}{1+e^{-2.21}} \\approx 0.90$  \n",
    "\n",
    "##### Compute $a^{(2)}_{0}$\n",
    "$z^{(2)}_0 = w^{(2)}_{0,0} \\cdot a^{(1)}_{0} + w^{(2)}_{0,1} \\cdot a^{(1)}_{1} + w^{(2)}_{0,2} \\cdot a^{(1)}_{2} = 0.28 \\cdot 0.88 + 0.42 \\cdot 0.90 + 0.27 \\cdot 1 = 0.89$   \n",
    "$a^{(2)}_{0} = g(z^{(2)}_0) = \\dfrac{1}{1+e^{-z^{(2)}_0}} = \\dfrac{1}{1+e^{-0.89}} \\approx 0.71$  \n",
    "\n",
    "##### Compute $a^{(2)}_{1}$\n",
    "$z^{(2)}_1 = w^{(2)}_{1,0} \\cdot a^{(1)}_{0} + w^{(2)}_{1,1} \\cdot a^{(1)}_{1} + w^{(2)}_{1,2} \\cdot a^{(1)}_{2} = 0.82 \\cdot 0.88 + 0.49 \\cdot 0.90 + 0.45 \\cdot 1 = 1.61$  \n",
    "$a^{(2)}_{1} = g(z^{(2)}_1) = \\dfrac{1}{1+e^{-z^{(2)}_1}} = \\dfrac{1}{1+e^{-1.61}} \\approx 0.83$   \n",
    "\n",
    "##### Compute $a^{(3)}_{0}$ and $a^{(3)}_{1}$ using Softmax activation funciton\n",
    "$z^{(3)}_0 = w^{(3)}_{0,0} \\cdot a^{(2)}_{0} + w^{(3)}_{0,1} \\cdot a^{(2)}_{1} + w^{(3)}_{0,2} \\cdot a^{(2)}_{2} = 0.32 \\cdot 0.71 + 0.83 \\cdot 0.83 + 0.70 \\cdot 1 = 1.62$  \n",
    "\n",
    "$z^{(3)}_1 = w^{(3)}_{1,0} \\cdot a^{(2)}_{0} + w^{(3)}_{1,1} \\cdot a^{(2)}_{1} + w^{(3)}_{1,2} \\cdot a^{(2)}_{2} = 0.70 \\cdot 0.71 + 0.72 \\cdot 0.83 + 0.59 \\cdot 1 = 1.68$  \n",
    "\n",
    "$\\hat{y}_0 = a^{(3)}_{0} = \\sigma(z^{(3)}_0) = \\dfrac{e^{1.62}}{e^{1.62} + e^{1.68}} = 0.49$  \n",
    "$\\hat{y}_1 = a^{(3)}_{1} = \\sigma(z^{(3)}_1) = \\dfrac{e^{1.68}}{e^{1.62} + e^{1.68}} = 0.51$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Error\n",
    "To calculate the error we will use the multi-class cross-entropy loss function instead of the binary cross-entropy loss function.  The multi-class cross-entropy loss function is a more generalized form and calculates exactly the same thing.  Instead of dealing with only two classes, the multi-class function can handle more 2 or more classes.  Note that we will never use the cost function in the training, but we have to compute the gradient of the cost function w.r.t weights and biases for their update.  So computing the loss is just to monitor how good we are doing.\n",
    "\n",
    "$C = -\\sum\\limits^{K}_{k=1}y_{k}\\log_2\\left(\\hat{y}_{k}\\right)$\n",
    "\n",
    "This function can easily be adjusted to take in account of more than one training example.\n",
    "\n",
    "$C = -\\dfrac{1}{m}\\sum\\limits^{m}_{i=1}\\sum\\limits^{K}_{k=1}y_{k}\\log_2\\left(\\hat{y}_{k}\\right)$\n",
    "\n",
    "\n",
    "\n",
    "Now let's calculate the error with our simplified loss function\n",
    "\n",
    "$C=  $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "$C = -\\dfrac{1}{m}\\sum\\limits^{m}_{i=1}\\sum\\limits^{K}_{k=1}y_{k}\\log_2\\left(\\hat{y}_{k}\\right)$\n",
    "\n",
    "Derivate of the cross-entropy error function.  \n",
    "$\\dfrac{\\part-\\dfrac{y_k}{\\hat{y}_k} + \\dfrac{1 - y_k}{1 - \\hat{y}_k}$  \n",
    "\n",
    "Calculate the weights, $w^{(3)}$  \n",
    "\n",
    "$\\dfrac{\\partial C}{\\partial w^{(3)}_{0,0}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
