{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Instead of using $y^{pred}_{j}$, use $\\hat{y}$ instead.  This change has been made for the cross entropy equation\n",
    "\n",
    "Let's define some notation to make the equations below more readable.  \n",
    "\n",
    "\n",
    "$g=$ activation function of output node (i.e. sigmoid function)  \n",
    "$h_{\\theta}\\left(x\\right)=g\\left(\\theta^{T}x\\right)$  \n",
    "$\\hat{y}=h_{\\theta}\\left(x\\right)$  \n",
    "$C=Cost\\left(\\hat{y},y\\right)$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Cost Function: Regression\n",
    "$C=\\dfrac{1}{2}\\sum\\limits_{j}\\left(\\hat{y}_{j}-y^{pred}_{j}\\right)^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Cost Function: Classification\n",
    "$C=-\\dfrac{1}{m}\\sum\\limits^{m}_{i=1}\\sum\\limits^{K}_{k=1}\\left[y^{(i)}_{k}\\log(\\hat{y}_k)+(1-y^{(i)}_{k})\\log(1-\\hat{y}_{k}) \\right]+\\dfrac{\\lambda}{2m} \\sum\\limits^{L-1}_{l=1}\\sum\\limits^{s_{l}}_{i=1}\\sum\\limits^{s_{l+1}}_{j=1}\\left(\\theta\\theta^{(l)}_{j,i}\\right)^{2}$  \n",
    "\n",
    "Note: This is the cost function that is used in the Andrew Ng course in Coursea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential Cost Function\n",
    "\n",
    "$C=\\tau\\exp\\left[\\dfrac{1}{\\tau}\\sum\\limits_{j}\\left(\\hat{y}_{j}-y^{pred}_{j}\\right)^{2}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hellinger Distance\n",
    "$C=\\dfrac{1}{\\sqrt{2}}\\sum\\limits_{j}\\left(\\sqrt{\\hat{y}_{j}}-\\sqrt{y^{pred}_{j}}\\right)^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kullback-Leibler Divergence\n",
    "$D_{KL}\\left(P\\|Q\\right) = \\sum\\limits_{i}P\\left(i\\right)\\ln{\\dfrac{P\\left(i\\right)}{Q\\left(i\\right)}}$  \n",
    "\n",
    "$C = \\sum\\limits_{j}\\hat{y}_{j}\\log{\\dfrac{\\hat{y}_{j}}{y^{pred}_{j}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Kullback-Leibler Divergence\n",
    "$C=\\sum\\limits_{j}\\hat{y}_{j}\\log{\\dfrac{\\hat{y}_{j}}{y^{pred}_{j}}}-\\sum\\limits_{j}\\hat{y}_{j}-\\sum\\limits_{j}y^{pred}_{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Itakura-Saito Distance\n",
    "$C=\\sum\\limits_{j}\\left(\\dfrac{\\hat{y}_{j}}{y^{pred}_{j}}-\\log{\\dfrac{\\hat{y}_{j}}{y^{pred}_{j}}}-1\\right)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
