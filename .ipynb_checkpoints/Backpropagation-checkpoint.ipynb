{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is the process by which a neural network learns.  Training data comes as input and output pairs $(x,y)$, where $x$ is the input and $y$ is the output.  $y$ is usually called the target.  The goal is to take the input $x$, pass it through the neural network (feedforward process), and output $\\hat{y}$ where $\\hat{y} = y$.  The problem is, if the parameters of the neural network aren't configured (trained) properly, than the neural network won't output the correct value, meaning $\\hat{y} \\neq y$.  Backpropagation is the process by which the parameters of the neural network are adjusted, so that when passing $x$ through the neural network, it will produce $\\hat{y}$ where $\\hat{y} = y$.  More concretly, if the training example is $(cat picture, yes)$, then when passing a cat picture through a neural network it should output yes.  If the neural network is not trained then most likely it will output no when indeed the picture is of a cat.  This would mean the neural network doesn't think the cat picture is of a cat.  The neural network would need to be adjusted through backpropagation so that the neural network would output the correct value, yes.  To learn this process, we will look at a very simple neural network and build on it.  Below is the network we'll be using.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"Images/simple_neural_network_1.png\" style=\"width:50%\" />\n",
    "  <figcaption><b>figure:&nbsp;&nbsp;</b>Simpe Neural Network - 4 Neurons</figcaption>\n",
    "</figure> \n",
    "\n",
    "In the figure above there is 1 input, 2 hidden, and 1 output layer\n",
    "\n",
    "> $a_n$ represents the activation function of the neuron  \n",
    "$w_n$ represents the weights of the network  \n",
    "$\\hat{y}$ is the value of the output node  \n",
    "\n",
    "The way backpropagation works, is to reduce the error/cost in the neural network as efficiently as possible.  This is done by computing the cost/error of the output and adjusting the weights and biases in the network going backwards through each layer.  So the cost of the above neural network would be dependent on all the weights and biases in the network which can be represented like $Cost = C(w_1, b_1, w_2, b_2, w_3, a_3)$.  This neural network is even to big to start describing the backpropagation process.  Let's focus on the last two neurons and their connection,\n",
    "\n",
    "<figure>\n",
    "  <img src=\"Images/simple_neural_network_2.png\" style=\"width:25%\" />\n",
    "  <figcaption><b>figure:&nbsp;&nbsp;</b>Simpe Neural Network - 1 Neuron</figcaption>\n",
    "</figure> \n",
    "\n",
    "The neurons are labeled $a^{L-1}$ and $a^{L}$, where $L$ is an index for a layer.  So since we have a 3 layer network, $L = 3$  \n",
    "Also note that $a^{(L)}$ and $\\hat{y}$ are the exact same thing.  Going forward, until otherwised noted, I will not use the $\\hat{y}$ notation, but only $a^{(L)}$ notation.  It will make the formulas easier to read and understand.\n",
    "\n",
    "During the feedforward process, a training example is passed forward through the network and produces an output, $a^{(L)}$.  In other words, $a^{(L)}$ is the neural networks guess.  So the first step in the backpropagation process is to compute the cost by computing the difference of the guess, $a^{(L)}$, from the target, $y$.  The formula to compute the cost is as follows,\n",
    "\n",
    "> $C_0 = (a^{(L)} - y)^{2}$\n",
    "\n",
    "$C_0$ is the cost of the train example and the subscript 0 means it's the cost of the first training example.\n",
    "\n",
    "Remember that,\n",
    "> $a^{(L)} = \\sigma(w^{(L)}a^{(L-1)}+b^{(L)})$\n",
    "\n",
    "It's better to think about $a^{L}$ as the following, so that the math and programming work out better,\n",
    "\n",
    "> $z^{(L)} = w^{(L)}a^{(L-1)}+b^{(L)}$  \n",
    "> $a^{(L)} = \\sigma(z^{(L)})$\n",
    "\n",
    "These equations can be thought of using a computational graph like below,\n",
    "\n",
    "<figure>\n",
    "  <img src=\"Images/computation_graph_derivative_1.png\" style=\"width:25%\" />\n",
    "  <figcaption><b>figure:&nbsp;&nbsp;</b>Computation Graph</figcaption>\n",
    "</figure> \n",
    "\n",
    "The graph above lets us visually look at how each parameter affects the cost $C_0$.  Now we have to introduce some Calculus.  Looking at how small changes of parameters affect the function can be done with Calculus using derivatives and in this case more specifically partial derivatives.  First let's look at how a small change in $w^{(L)}$ affects/changes $C_0$.  A small change in $w^{(L)}$ is represented as $\\partial w^{(L)}$ and a small change in $C_0$ is represented as $\\partial C_0$.  The $\\partial$ notation can be read as \"del\".  A better way of saying this is, what is the derivative of $C_0$ with respect to $w^{(L)}$ and it can be represented as a ratio like so,\n",
    "\n",
    "> $\\dfrac{\\partial C_0}{\\partial w^{(L)}}$\n",
    "Note: This is read \"The derivative of C subcript zero with respect to W L\"\n",
    "\n",
    "A small change to $\\partial w^{(L)}$ causes a small change to $\\partial z^{(L)}$ which causes a change to $\\partial a^{(L)}$ which causes a change to $\\partial C_0$.\n",
    "\n",
    "So let's break this out by looking at, how a small change to $\\partial w^{(L)}$ changes $\\partial z^{(L)}$.\n",
    "\n",
    "> $\\dfrac{\\partial C_0}{\\partial w^{(L)}} = \\dfrac{\\partial z^{(L)}}{\\partial w^{(L)}}$\n",
    "\n",
    "then by looking at how a small change to $\\partial z^{(L)}$ changes $\\partial a^{(L)}$\n",
    "\n",
    "> $\\dfrac{\\partial C_0}{\\partial w^{(L)}} = \\dfrac{\\partial z^{(L)}}{\\partial w^{(L)}} \\dfrac{\\partial a^{(L)}}{\\partial z^{(L)}}$\n",
    "\n",
    "then finally looking at how a small change to $\\partial a^{(L)}$ changes $\\partial C_0$\n",
    "\n",
    "> $\\dfrac{\\partial C_0}{\\partial w^{(L)}} = \\dfrac{\\partial z^{(L)}}{\\partial w^{(L)}} \\dfrac{\\partial a^{(L)}}{\\partial z^{(L)}} \\dfrac{\\partial C_0}{\\partial a^{(L)}}$\n",
    "\n",
    "This final equation that we built up, is the [Chain Rule](https://en.wikipedia.org/wiki/Chain_rule).  If you know Calculus, the Chain Rule describes how to compute the derivative of a function of functions or a composite function, i.e.) $f(g(x))$. There are a lot of articles on the Chain Rule, so I won't be going into the details here, but basically it says,\n",
    "\n",
    "$\\dfrac{d}{dx} f(g(x)) = \\dfrac{d}{dx} f(g(x)) \\dfrac{d}{dx} g(x) $\n",
    "\n",
    "or better notation is to use the prime notation $^{\\prime}$ to represent derivatives.  So to make the formula more readable, you'll often see it like the following,\n",
    "\n",
    "$f^{\\prime}(g(x)) = f^{\\prime}(g(x)) g^{\\prime}(x) $\n",
    "\n",
    "If you look at how the forumla for the computational graph above was derived, you'll see that it's just the chain rule.\n",
    "\n",
    "Now that we have the formula for the $\\dfrac{\\partial C_0}{\\partial w^{(L)}}$, let's now compute the relevant derivates.  There're a lot of symbols on the page, so to re-group and limit the confusion, I listed the computed formulas so far.\n",
    "\n",
    "$\\dfrac{\\partial C_0}{\\partial w^{(L)}} = \\dfrac{\\partial z^{(L)}}{\\partial w^{(L)}} \\dfrac{\\partial a^{(L)}}{\\partial z^{(L)}} \\dfrac{\\partial C_0}{\\partial a^{(L)}}$\n",
    "\n",
    "$C_0 = (a^{(L)} - y)^{2}$  \n",
    "\n",
    "$z^{(L)} = w^{(L)}a^{(L-1)}+b^{(L)}$    \n",
    "\n",
    "$a^{(L)} = \\sigma(z^{(L)})$  \n",
    "\n",
    "Now let's take compute the derivates of the first formula starting from the last term.  The functions we will be deriving are the last 3.\n",
    "\n",
    "$\\dfrac{\\partial C_0}{\\partial a^{(L)}} = 2(a^{(L)} - 1)$  \n",
    "$\\dfrac{\\partial a^{(L)}}{\\partial z^{(L)}} = \\sigma^{\\prime}(z^{(L)})$  \n",
    "$\\dfrac{\\partial z^{(L)}}{\\partial w^{(L)}} = a^{(L-1)}$  \n",
    "\n",
    "Therefore,\n",
    "\n",
    "$\\dfrac{\\partial C_0}{\\partial w^{(L)}} = \\dfrac{\\partial z^{(L)}}{\\partial w^{(L)}} \\dfrac{\\partial a^{(L)}}{\\partial z^{(L)}} \\dfrac{\\partial C_0}{\\partial a^{(L)}} = a^{(L-1)}  \\sigma^{\\prime}(z^{(L)})  2(a^{(L)} - 1)$\n",
    "\n",
    "We have nearly everything we need to compute the cost for small changes in $w^{L}$, but remember this was only for one training example.  This is way we represented the cost function as $C_0$ where the subscript 0 means first training example.  The total cost is the average cost of all computed costs in the whole training set.  So the formula to compute that is as follows\n",
    "\n",
    "$\\dfrac{\\partial C}{\\partial w^{(L)}}  = \\dfrac{1}{n} \\sum\\limits_{k=0}^{n-1} \\dfrac{\\partial C_k}{\\partial w^{(L)}}$\n",
    "\n",
    "Let's break this down,\n",
    "\n",
    "$n$ is the number of training examples.  This formula is summing up all the costs for each training example like so, $\\sum\\limits_{k=0}^{n-1} \\dfrac{\\partial C_k}{\\partial w^{(L)}}$ and then divides by the number of training examples using this term, $\\dfrac{1}{n}$.  It's an average of all costs over the entire training set.\n",
    "\n",
    "Looking back at the computational graph above, these calculations were only for the $w^{(L)}$ parameter.  Luckily there not much difference with the other parameters and most of the work is already done.  \n",
    "\n",
    "$\\dfrac{\\partial C_0}{\\partial w^{(L)}} = a^{(L-1)}  \\sigma^{\\prime}(z^{(L)})  2(a^{(L)} - 1)$  \n",
    "$\\dfrac{\\partial C_0}{\\partial b^{(L)}} = a^{(L-1)}  \\sigma^{\\prime}(z^{(L)})  2(a^{(L)} - 1)$  \n",
    "$\\dfrac{\\partial C_0}{\\partial w^{(L)}} = a^{(L-1)}  \\sigma^{\\prime}(z^{(L)})  2(a^{(L)} - 1)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
