{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Pacakages and Set Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "\n",
    "    train_dataset = h5py.File('D:/Datasets/train_catvnoncat.h5', \"r\")\n",
    "\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "\n",
    "\n",
    "    test_dataset = h5py.File('D:/Datasets/test_catvnoncat.h5', \"r\")\n",
    "\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "\n",
    "    \n",
    "\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    \n",
    "\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements the sigmoid activation in numpy\n",
    "#\n",
    "# Arguments:\n",
    "#    Z -- numpy array of any shape\n",
    "#\n",
    "# Returns:\n",
    "#    A -- output of sigmoid(z), same shape as Z\n",
    "#    cache -- returns Z as well, useful during backpropagation\n",
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Backwards Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the backward propagation for a single SIGMOID unit.\n",
    "#\n",
    "# Arguments:\n",
    "#    dA -- post-activation gradient, of any shape\n",
    "#    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "#\n",
    "# Returns:\n",
    "#    dZ -- Gradient of the cost with respect to Z\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the RELU function.\n",
    "#\n",
    "# Arguments:\n",
    "#    Z -- Output of the linear layer, of any shape\n",
    "#\n",
    "# Returns:\n",
    "#    A -- Post-activation parameter, of the same shape as Z\n",
    "#    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = Z \n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Backwards Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the backward propagation for a single RELU unit.\n",
    "#\n",
    "# Arguments:\n",
    "#    dA -- post-activation gradient, of any shape\n",
    "#    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "#\n",
    "# Returns:\n",
    "#    dZ -- Gradient of the cost with respect to Z\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "\n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to predict the results of a  L-layer neural network.\n",
    "#    \n",
    "# Arguments:\n",
    "#    X -- data set of examples you would like to label\n",
    "#    parameters -- parameters of the trained model\n",
    "#    \n",
    "# Returns:\n",
    "#    p -- predictions for the given dataset X\n",
    "def predict(X, y, parameters):\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Mismatched Pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots images where predictions and truth were different.\n",
    "#    X -- dataset\n",
    "#    y -- true labels\n",
    "#    p -- predictions\n",
    "def print_mislabeled_images(classes, X, y, p):\n",
    "    a = p + y\n",
    "    mislabeled_indices = np.asarray(np.where(a == 1))\n",
    "    plt.rcParams['figure.figsize'] = (40.0, 40.0) # set default size of plots\n",
    "    num_images = len(mislabeled_indices[0])\n",
    "    for i in range(num_images):\n",
    "        index = mislabeled_indices[1][i]\n",
    "        \n",
    "        plt.subplot(2, num_images, i + 1)\n",
    "        plt.imshow(X[:,index].reshape(64,64,3), interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Prediction: \" + classes[int(p[0,index])].decode(\"utf-8\") + \" \\n Class: \" + classes[y[0,index]].decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments:\n",
    "#    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "#    \n",
    "# Returns:\n",
    "#    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "#    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "#    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the linear part of a layer's forward propagation.\n",
    "#\n",
    "# Arguments:\n",
    "#    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "#    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "#    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "#\n",
    "# Returns:\n",
    "#    Z -- the input of the activation function, also called pre-activation parameter \n",
    "#    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear-Activation-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "#\n",
    "# Arguments:\n",
    "#    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "#    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "#    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "#    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "#\n",
    "# Returns:\n",
    "#    A -- the output of the activation function, also called the post-activation value \n",
    "#    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "#             stored for computing the backward pass efficiently\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L_Model_Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "#    \n",
    "# Arguments:\n",
    "#    X -- data, numpy array of shape (input size, number of examples)\n",
    "#    parameters -- output of initialize_parameters_deep()\n",
    "#    \n",
    "# Returns:\n",
    "#    AL -- last post-activation value\n",
    "#    caches -- list of caches containing:\n",
    "#                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the cost function defined by equation (7).\n",
    "#\n",
    "# Arguments:\n",
    "#    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "#    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "#\n",
    "# Returns:\n",
    "#    cost -- cross-entropy cost\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1/m) * np.sum(np.dot(Y, np.log(AL).T) + np.dot(1-Y, np.log(1-AL).T))\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear-Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "#\n",
    "# Arguments:\n",
    "#    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "#    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "#\n",
    "# Returns:\n",
    "#    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "#    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "#    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m)*np.dot(dZ, A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear-Activation-Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "#  \n",
    "# Arguments:\n",
    "#    dA -- post-activation gradient for current layer l \n",
    "#    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "#    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "#    \n",
    "# Returns:\n",
    "#    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "#    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "#    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L-Model-Backwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "#    \n",
    "# Arguments:\n",
    "#    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "#    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "#    caches -- list of caches containing:\n",
    "#                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "#                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "#    \n",
    "# Returns:\n",
    "#    grads -- A dictionary with the gradients\n",
    "#             grads[\"dA\" + str(l)] = ... \n",
    "#             grads[\"dW\" + str(l)] = ...\n",
    "#             grads[\"db\" + str(l)] = ... \n",
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients.\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "\n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update parameters using gradient descent\n",
    "#    \n",
    "# Arguments:\n",
    "#    parameters -- python dictionary containing your parameters \n",
    "#    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "#    \n",
    "# Returns:\n",
    "#    parameters -- python dictionary containing your updated parameters \n",
    "#                  parameters[\"W\" + str(l)] = ... \n",
    "#                  parameters[\"b\" + str(l)] = ...\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L-Layer-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "#    \n",
    "# Arguments:\n",
    "#    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "#    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "#    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "#    learning_rate -- learning rate of the gradient descent update rule\n",
    "#    num_iterations -- number of iterations of the optimization loop\n",
    "#    print_cost -- if True, it prints the cost every 100 steps\n",
    "#    \n",
    "# Returns:\n",
    "#    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. (â‰ˆ 1 line of code)\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [12288, 20, 7, 5, 1] #  4-layer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (12288, 209)\n",
      "test_x's shape: (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693148\n",
      "Cost after iteration 100: 0.678011\n",
      "Cost after iteration 200: 0.667600\n",
      "Cost after iteration 300: 0.660422\n",
      "Cost after iteration 400: 0.655458\n",
      "Cost after iteration 500: 0.652013\n",
      "Cost after iteration 600: 0.649616\n",
      "Cost after iteration 700: 0.647942\n",
      "Cost after iteration 800: 0.646770\n",
      "Cost after iteration 900: 0.645947\n",
      "Cost after iteration 1000: 0.645368\n",
      "Cost after iteration 1100: 0.644961\n",
      "Cost after iteration 1200: 0.644673\n",
      "Cost after iteration 1300: 0.644469\n",
      "Cost after iteration 1400: 0.644325\n",
      "Cost after iteration 1500: 0.644223\n",
      "Cost after iteration 1600: 0.644151\n",
      "Cost after iteration 1700: 0.644100\n",
      "Cost after iteration 1800: 0.644063\n",
      "Cost after iteration 1900: 0.644037\n",
      "Cost after iteration 2000: 0.644019\n",
      "Cost after iteration 2100: 0.644006\n",
      "Cost after iteration 2200: 0.643997\n",
      "Cost after iteration 2300: 0.643990\n",
      "Cost after iteration 2400: 0.643985\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6555023923444976\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3400000000000001\n"
     ]
    }
   ],
   "source": [
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mislabeled Pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mislabeled_images(classes, test_x, test_y, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
