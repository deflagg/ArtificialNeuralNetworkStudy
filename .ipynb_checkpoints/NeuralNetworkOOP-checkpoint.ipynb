{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network Using Object Oriented Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm creating this notebook to better understand how neural networks work.  One way to better understand anything is to build it from scratch and that's what I will do here.  I will try to provide a clear explaination and associated code with a concrete example.  By the end of this notebook, there will be a clear explaination of the feed foward and backpropagation processes and a full neural network written in Python using object oriented programming.  I chose to use object oriented programming instead of functional programming with matrices to ensure less ambiguity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A artificial neural network (ANN) is a mathematical function.  Basically this means that it represents a function that takes some inputs and computes an output(s).  In fact, neural networks are said to be able to compute any function.  Suppose we are given the function below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"Images/function.png\" style=\"width:20%\" />\n",
    "  <figcaption><b>figure:&nbsp;&nbsp;</b>Random Function</figcaption>\n",
    "</figure> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter what the function, there's guaranteed to be a  neural network so that for every possible input, x, the value f(x) (or some close approximation) is output from the network.  Neural networks have a _universal_ nature to them, in that they can universally approximate any function, whether it's a 2-Dimensional, 3-Dimensional, or up to N-Dimensional function.  The function above is a 2-D function since it's represented only by x and y points.  This property of neural networks is described by the [Universal Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) and it can be called a universal function approximator.  The word approximation means that the neural network approximates a function. When a neural network can approximate a function, it is said that the neural network models the function or the data.  A neural network has the ability to model the above function exactly, but it will take a long time to compute and usually an approximate is just as good.  Check out this e-book for more information, [A visual proof that neural nets can compute any function, by Michael Nielsen](http://neuralnetworksanddeeplearning.com/chap4.html)\n",
    "\n",
    "> _A feedforward network with a single layer is sufficient to represent any function, but the layer may be infeasibly large and may fail to learn and generalize correctly.\n",
    "— Ian Goodfellow, DLB_\n",
    "\n",
    "> _Introducing non-linearity via an activation function allows us to approximate any function. It’s quite simple, really. — Elon Musk_\n",
    "\n",
    "For a neural network to model the function in the above figure it would need to be _trained_ first.  To train a neural network means that it will look at training data and based on that it'll adjust it's properties until the desired output is given.  More concretely, to train a neural network to model the above function, the set of $(x,y)$ points (called the _training set_) in the function above will be passed through the graph and the properties of the network would be udated accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we basically know what a neural network does, let's look at how it's graphically represented.  Neural networks are structured by a series of layers and the layers are composed of neurons.  Each neuron can take 1 or more inputs and return an output.  Below is a graphical representation of a neural network.  The circles represet the neurons and the lines represent inputs and outputs.  As you can see, neurons in the layer are connected to all neurons of the next layer.  Each layer is also annotated with the type of layer it is e.g.) Hidden Layer 1.  Each connection also has a weight, $w_n$, applied to it.  The weights are the neural network properties that will be changed during training.\n",
    "\n",
    "We're going to use a neural network with 1 input layer, 2 hidden layers, and 1 output layer to make up a 3-layered network.  Remember, the input layer isn't usually counted when counting the number of layers in a neural network.  Below is the graphical representation of our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"Images/deep_neural_network_1.png\" style=\"width:50%\" />\n",
    "  <figcaption><b>figure:&nbsp;&nbsp;</b>3-Layer Neural Network</figcaption>\n",
    "</figure> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each neuron also has a structure.  A neuron takes a set of inputs and returns an output.  The output is computed be first taking the weighted sum of the inputs, plus the bias (we'll talk about bias later).  For now think of the bias as another weight.  The output of this calculation is then used as input to the activation function.  There are many different activation functions that can be used. I wrote about many of the main ones in [Activation Functions](ActivationFunctions.ipynb).  The computation is below:\n",
    "\n",
    "> $z = (\\sum\\limits_{i=1}^n x_i \\cdot w_i) + b$  \n",
    "$output = activation(z)$\n",
    "\n",
    "And the next figure below is a diagram of each neuron (circle) above in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"Images/neuron.png\" style=\"width:50%\" />\n",
    "  <figcaption><b>figure:&nbsp;&nbsp;</b>Neuron</figcaption>\n",
    "</figure> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start writing code, let's decompose the learning process into steps.\n",
    "\n",
    "1. **Initialize the model**  \n",
    "To initialize the model is to set the weights $w_n$ and bias $b$ values.  The initialization is random, meaning these parameters are set to any random value. There're some rules and techniques for initializing the model so that it learns faster which can be viewed in the [Initialization Notebook](NeuralNetworkInitialization.ipynb)  \n",
    "\n",
    "Repeat\n",
    "> 2. **Feedforward**  \n",
    "As the name suggests, this step moves the data from the input layer, through the hidden layers, and produces an output(s).  At each node, the weighted sum of the inputs and weights are calculated and then that value is used to compute the activation function.  Refer to the _**figure:** Neuron_ above.\n",
    "1. **Compute loss**  \n",
    "After an output(s) is calculated by the feedforward step, then the total error is calculated.\n",
    "1. **Backpropagation**  \n",
    "This is the step where the magic happens and the network learns. I wrote about how backpropagation works in [Backpropagation](Backpropagation.ipynb).\n",
    "1. **Update weights**  \n",
    "\n",
    "while ((maximum  number of iterations < than specified) AND  \n",
    "          (Error Function is > than specified))\n",
    "\n",
    "These are the general steps used to train a neural network to approximate a function/model for a given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development\n",
    "\n",
    "Let's first start with developing a neuron.  This is not developed in an optimized fashion, but in a way it's understandable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Pacakages and Set Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the matplotlib backend\n",
    "%matplotlib inline\n",
    "\n",
    "# Set default plot parameters\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Ensure we get the same random numbers each time by using a constant seed value. (for debugging purposes only)\n",
    "random.seed(1)\n",
    "\n",
    "#\n",
    "# ReLU function\n",
    "#\n",
    "def relu_forward(x):\n",
    "    y = max(0, x)\n",
    "    return y\n",
    "\n",
    "#\n",
    "# Derivative of ReLU function\n",
    "#\n",
    "def relu_backward(x):\n",
    "    if x > 0:\n",
    "        y = 1\n",
    "    elif x <= 0:\n",
    "        y = 0\n",
    "    return y\n",
    "\n",
    "#\n",
    "# Sigmoid function\n",
    "#\n",
    "def sigmoid_forward(x):\n",
    "    y = 1/(1+math.e**(-x))\n",
    "    return y\n",
    "\n",
    "#\n",
    "# Derivative of Sigmoid function\n",
    "#\n",
    "def sigmoid_backward(x):\n",
    "    y = sigmoid(x) * (1 - sigmoid(x))\n",
    "    return y\n",
    "\n",
    "# group the activation functions and their derivatives so \n",
    "# so they can be referenced during the feed forward and backpropagation processes\n",
    "sigmoid = [sigmoid_forward, sigmoid_backward]\n",
    "relu = [relu_forward, relu_backward]\n",
    "\n",
    "\n",
    "#\n",
    "# Neuron\n",
    "#\n",
    "class Neuron:\n",
    "    def __init__(self, activation_function, activation_function_derivative):\n",
    "        self.input_neurons = []\n",
    "        self.output = 0\n",
    "        self.weights = []\n",
    "        self.bias = 0\n",
    "        self.activation = activation_function\n",
    "        self.activation_derivative = activation_function_derivative\n",
    "        \n",
    "    \n",
    "    def _calculate_weighted_sum(self):\n",
    "        weighted_sum = 0\n",
    "        \n",
    "        # z = Σxi+wi\n",
    "        for i in range(len(self.input_neurons)):\n",
    "            weighted_sum += self.input_neurons[i].output * self.weights[i]\n",
    "            \n",
    "        # z = z + b\n",
    "        weighted_sum += self.bias\n",
    "        \n",
    "        return weighted_sum\n",
    "    \n",
    "    def calculate_output(self):\n",
    "\n",
    "        # z\n",
    "        weighted_sum = self._calculate_weighted_sum()\n",
    "        \n",
    "        # activation(z)\n",
    "        self.output = self.activation(weighted_sum)\n",
    "        return self.output\n",
    "    \n",
    "    # Calculate the partial derivative of the cost with respect to the output, ∂𝐶/∂𝑎\n",
    "    # Since our cost method, _calculate_cost, is defined as (1/2)(𝑎(𝐿) − 𝑦)^2,\n",
    "    # the derivate is (𝑎(𝐿) − 𝑦)\n",
    "    def calculate_pd_c_wrt_a(self, target):\n",
    "        return (self.output - target)\n",
    "    \n",
    "    # Calculate ∂𝑎 with respect to ∂𝑧, ∂𝑎/∂𝑧\n",
    "    # Made the activation_derivative dynamic because we need to be able to use different \n",
    "    # activiation functions.  Activation functions and their derivatives are defined above\n",
    "    def calculate_pd_a_wrt_z(self):\n",
    "        return self.activation_derivative(self.output)\n",
    "    \n",
    "    # Calculate ∂𝐶 with respect to ∂𝑧, ∂𝐶/∂𝑧\n",
    "    # The terms/equation, ∂𝐶/∂𝑎 * ∂𝑎/∂𝑧, is common in the set of Chain Rule equations\n",
    "    def calculate_pd_c_wrt_z(self):\n",
    "        return self.calculate_pd_c_wrt_a() * self.calculate_pd_a_wrt_z()\n",
    "\n",
    "    def calculate_pd_z_wrt_w(self):\n",
    "        return None\n",
    "    \n",
    "    def backpropagation(self):\n",
    "        \n",
    "        # Initialize an array to cache the new weights\n",
    "        # Creates an array where the number of rows equals the number of layers\n",
    "        #   and the number of columns equal the number of weights.\n",
    "        # The number of weights for each layer is calculated by multiplying the number of neurons in the layer by the number of weights for \n",
    "        # one neuron in the same layer, because the number of weights in each neuron for a given layer are the same.\n",
    "        updated_weights = [[0] * (layer.neuron[0].weights * len(layer.neurons)) for layer in self.layers]\n",
    "        \n",
    "        for layer_index in range(len(self.layers) - 1, -1, -1):\n",
    "            # Cache ∂𝐶/∂𝑧 = ∂𝐶/∂𝑎 * ∂𝑎/∂𝑧\n",
    "            pd_c_wrt_z = [0] * len(self.layers[layer_index].neurons)\n",
    "            \n",
    "            for neuron_index in range(len(self.layers[layer_index].neurons) - 1):\n",
    "                \n",
    "                for weight_index in range(len(self.layers[layer_index].neurons[neuron_index].weights) - 1):\n",
    "                    \n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "#\n",
    "# Layer of Neurons\n",
    "#\n",
    "class Layer:\n",
    "    def __init__(self, neurons):\n",
    "        self.neurons = neurons\n",
    "    \n",
    "#\n",
    "# NeuralNetwork\n",
    "#\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers_dimensions):\n",
    "        # 1. Initialize the model\n",
    "        self.initialize()\n",
    "        \n",
    "    def initialize(self):\n",
    "        self._build_neural_network()\n",
    "                    \n",
    "    def _build_neural_network(self):\n",
    "        # Create layers with the specified number of neurons\n",
    "        self.layers = [Layer([Neuron(layer[1][0],layer[1][1]) for n in range(layer[0])]) for layer in layers_dimensions]\n",
    "        \n",
    "        # Iterate through the layers and connect the neurons from the previous layer to the current layer\n",
    "        for layer_index in range(1, len(self.layers)):\n",
    "            for node in self.layers[layer_index].neurons:\n",
    "                node.input_neurons = [neuron for neuron in self.layers[layer_index - 1].neurons]\n",
    "                node.weights = [random.random() for neuron in self.layers[layer_index - 1].neurons]\n",
    "                node.bias = 0\n",
    "                \n",
    "    def print_self(self):\n",
    "        # Print out neural network\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            \n",
    "            if(l == 0):\n",
    "                print(\"INPUT LAYER \" + str(l))\n",
    "            elif(l == len(self.layers) - 1):\n",
    "                print(\"OUTPUT LAYER \" + str(l))\n",
    "            else:\n",
    "                print(\"LAYER \" + str(l))\n",
    "                \n",
    "            for n, neuron in enumerate(layer.neurons):\n",
    "                print(\"   neuron: \" + str(n))\n",
    "                print(\"      activation: \" + neuron.activation.__name__ if neuron.activation != None else \"\") \n",
    "                print(\"      bias: \" + str(neuron.bias))\n",
    "                print(\"      weights: \" + str(neuron.weights))\n",
    "                print(\"      output: \" + str(neuron.output))\n",
    "    \n",
    "    def feed_forward(self, input_data):\n",
    "        \n",
    "        # Set inputs into the neural network input layer\n",
    "        for index, neuron in enumerate(self.layers[0].neurons):\n",
    "            neuron.output = input_data[index]\n",
    "        \n",
    "        # Compute the output of each node in each layer\n",
    "        for layer_index, layer in enumerate(self.layers):\n",
    "            if layer_index == 0:\n",
    "                continue\n",
    "                \n",
    "            for node in layer.neurons:\n",
    "                node.calculate_output()\n",
    "    \n",
    "    def calculate_total_cost(self, training_output):\n",
    "        total_cost = 0\n",
    "\n",
    "        for o in range(len(training_output)):\n",
    "            output = self.layers[len(self.layers) - 1].neurons[o].output\n",
    "            target_output = training_output[o]\n",
    "            total_cost += self._calculate_cost(output, target_output)\n",
    "            \n",
    "        return total_cost\n",
    "    \n",
    "    # Calculates (1/2)(𝑎(𝐿) − 𝑦)^2\n",
    "    # The derivative of this function is calculate_pd_errors_wrt_output\n",
    "    def _calculate_cost(self, output, target_output):\n",
    "            return 0.5 * ((output - target_output) ** 2)\n",
    "    \n",
    "    \n",
    "    def backpropagation(self):\n",
    "        for l in range(len(self.layers) - 1):\n",
    "            for n, neuron in enumerate()\n",
    "     \n",
    "    # Uses online learning, ie updating the weights after each training case\n",
    "    def train(self, training_inputs, training_outputs):\n",
    "        for training_input, training_output in zip(training_inputs, training_outputs):\n",
    "            # 2 Feedforward\n",
    "            self.feed_forward(training_input)\n",
    "\n",
    "            # 3 Calculate cost of outputs for graphing/monitoring\n",
    "            self.calculate_total_cost(training_output)\n",
    "            \n",
    "            # 4 Backpropagation\n",
    "            self.backpropagation()\n",
    "            \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT LAYER 0\n",
      "   neuron: 0\n",
      "\n",
      "      bias: 0\n",
      "      weights: []\n",
      "      output: 3\n",
      "   neuron: 1\n",
      "\n",
      "      bias: 0\n",
      "      weights: []\n",
      "      output: 8\n",
      "LAYER 1\n",
      "   neuron: 0\n",
      "      activation: relu_forward\n",
      "      bias: 0\n",
      "      weights: [0.13436424411240122, 0.8474337369372327]\n",
      "      output: 7.182562627835065\n",
      "   neuron: 1\n",
      "      activation: relu_forward\n",
      "      bias: 0\n",
      "      weights: [0.763774618976614, 0.2550690257394217]\n",
      "      output: 4.3318760628452155\n",
      "LAYER 2\n",
      "   neuron: 0\n",
      "      activation: relu_forward\n",
      "      bias: 0\n",
      "      weights: [0.49543508709194095, 0.4494910647887381]\n",
      "      output: 5.505633125085929\n",
      "   neuron: 1\n",
      "      activation: relu_forward\n",
      "      bias: 0\n",
      "      weights: [0.651592972722763, 0.7887233511355132]\n",
      "      output: 8.096759139429462\n",
      "OUTPUT LAYER 3\n",
      "   neuron: 0\n",
      "      activation: sigmoid_forward\n",
      "      bias: 0\n",
      "      weights: [0.0938595867742349, 0.02834747652200631]\n",
      "      output: 0.6783674023802385\n",
      "   neuron: 1\n",
      "      activation: sigmoid_forward\n",
      "      bias: 0\n",
      "      weights: [0.8357651039198697, 0.43276706790505337]\n",
      "      output: 0.9996981947463441\n"
     ]
    }
   ],
   "source": [
    "\n",
    "layers_dimensions = [[2,[None,None]],[2,relu],[2,relu],[2,sigmoid]]\n",
    "#[Layer([Neuron(sigmoid, _, _) for n in range(layer[0])]) for layer in layers]\n",
    "\n",
    "data_in = [[3, 8]]\n",
    "data_out = [[0.3, 1.2]]\n",
    "neural_network = NeuralNetwork(layers_dimensions)\n",
    "neural_network.train(data_in, data_out)\n",
    "\n",
    "neural_network.print_self()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
