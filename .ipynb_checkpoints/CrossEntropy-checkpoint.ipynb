{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Entropy Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Entropy (Physics Domain)\n",
    "Neural Networks come in two flavors, logistical regression and classification.  It's common for classification neural networks to use the cross-entropy loss function to calculate the cost during training.  Why is this loss function used and what does it measure?  To understand why this loss function is used, let's start by defining the word entropy. Entropy is a concept that comes from physics, specifically from the second law of thermodynamics.  Entropy explains why all things decay/die, buildings crumble, mountains erode, and why your bedroom takes work to keep it organized.  More formally, entropy is the measure of disorder in a system.  This is typically the one-liner people will give you if you ask what entropy is, but it's confusing.  It doesn't help with understaning entropy.  What does disorder in a system mean?  Below are a few sections to gain a better intuition of what entropy is.\n",
    "\n",
    "#### Intuition #1\n",
    "During the industrial revolution engines such as the [Thomas Savery's (1698)](https://en.wikipedia.org/wiki/Thomas_Savery), the [Newcomen engine (1712)](https://en.wikipedia.org/wiki/Newcomen_engine), [Cugnot steam tricycle (1769)](https://en.wikipedia.org/wiki/Steam_tricycle), and the [Stirling engine (1816)](https://en.wikipedia.org/wiki/Stirling_engine) were heat-powered engines.  These engines were inefficient, converting approximately 2% of the input energy into useful [work output](https://en.wikipedia.org/wiki/Work_output).  In order to develop more efficient engines, physicists researched this energy loss.  In 1865 Rudolf Clausius, a german physicist and mathematician, coined the term entropy.  If you recognize the name, he's also the same physicist who first stated the second law of thermodynamics.  To describe entropy let's focus on how the Stirling engine works.  Below are a couple pictures of one type of Stirling engine.  For your edification, the engines in the two pictures below are \"Gamma\" type Stirling engines.  There're two other types Alpha and Beta, read about them on [Wikipedia](https://en.wikipedia.org/wiki/Stirling_engine).  \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <figure style=\"text-align:center;\">\n",
    "              <img src=\"Images/stirling_engine2.gif\" style=\"width:100%\" />\n",
    "            </figure>\n",
    "        </td>\n",
    "        <td>\n",
    "            <figure style=\"text-align:center;\">\n",
    "              <img src=\"Images/stirling_engine3.gif\" style=\"width:100%\" />\n",
    "            </figure>\n",
    "        </td>\n",
    "        <td>\n",
    "            <figure style=\"text-align:center;\">\n",
    "              <img src=\"Images/stirling_engine.png\" style=\"width:57%\" />\n",
    "            </figure>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Basically, the Stirling engine works off a difference in heat.  This engine is considered a low powered engine and can run off the heat from your hand.  The base of the engine is a cylindar with two metal plates that cover the top and the bottom.  In the cylindar resides a block of foam that can move up and down.  The man in the picture has his hand in contact with the bottom metal plate and therefore it heats up, while the top metal plate is at room temperature.  The air in the cylindar that is in contact with the bottom plate heats up and expands.  This temperature difference is what drives the wheel.  Below are the steps that make the Stirling engine run.\n",
    "\n",
    "1. Let's start with the foam at the top of the cylindar.\n",
    "2. Heat the bottom metal plate.\n",
    "3. The air in the cylindar in contact with the metal plate also heats up.\n",
    "4. When the air heats up it expands which pushes the piston up.\n",
    "5. As the piston is pushed up, the wheel moves and pushes the foam piece down.\n",
    "6. The air in the cylindar is now in contact with the cold top plate.\n",
    "7. When the air is cooled it contracts which pulls the piston down.\n",
    "8. As the piston is pulled down, the wheel moves and pulls the foam piece up.\n",
    "9. Then it repeats from step 2.\n",
    "\n",
    "You may be wondering what this has to do with entropy, but hang on we're getting there.  The most important thing to remember about this engine is that it doesn't run off of heat, but off a difference in temperature.  Imagine two metal bars, one hot and the other relatively cold.  When the two metal bars touch, as you would expect, the cold bar gets hotter.  This happens until both metal bars are the same temperature.  The heat energy is evenly distributed across both bars and nothing else happens.  Now imagine connecting the cold metal bar to the top metal plate and the hot metal bar to the bottom metal plate on the Stirling engine.  The wheel will begin to turn because of the difference in temperature.  After the heat in both metal bars are the same (evenly distributed) the engine will stop turning because there's no temperature difference.  The most important concept to understand here is that energy (heat) is only useful when it's clumped together.  When energy is used, it's spread out and therefore becomes less useful.  As stated earlier, studying the inefficiencies in engines during the industrial revolution led to the coining of the term entropy.  Instead of defining entropy as the measure of disorder in a system, we can now define it as the measure of how spread out the engery is.  Entropy is always moving from a low state (ordered/clumped) to a high state (disordered/evenly distributed). Thinking about this and going back to when the two metal bars are touching and the heat is distributed evenly, you will never see one of the bars steal heat from the other bar.  This doesn't happen in nature. Just like with coal, when you burn coal (clumped up energy) it produces heat (spread out energy).  You never seen the heat getting turned back into coal.\n",
    "\n",
    "#### Intuition #2\n",
    "Let's look at another example of entropy.  Image you have a cardboard box full of red and blue balls, with all the red balls on one side and all the blue balls on the other side.  Now shake the box, and as you would expect, the red and blue balls would be all mixed up (disordered). Shake the box again and the balls will still remain mixed up (disordered).  We can be very sure that after shaking the box, the red balls will most likely never end up on one side while the blue balls end up on the other.  The probablity of this happening is infinitesimally small, but not zero.  When the red and blue balls are on opposite sides of box, it is said that it's in a state of low entropy.  Therefore, when the balls are disordered, it is said it's in a state of high entropy.  \n",
    "\n",
    "#### Intuition #3\n",
    "This is not an original idea, but was first put forth by [Brian Cox](https://en.wikipedia.org/wiki/Brian_Cox_%28physicist%29), a professor of particle physics in the School of Physics and Astronomy at the University of Manchester.  A pile of sand is composed of individual sand grains.  The sand grains in a pile of sand can be re-arranged an \"infinite\" number of ways and it will still be a pile of sand.  If a sandcastle is built from the pile of sand, then there's less ways the sand can be re-arranged and have it look like the same sandcastle.  So a pile of sand has more entropy then a sandcastle because it has more disorder.  Another way to say this is, the sand in a pile of sand can be re-arranged in many more ways then the sand in the sandcastle and have it remain the same.  Physics says that all things move from a state of low entropy to a state of high entropy over time, which some physicists think is the arrow of time itself.  This is why a sandcastle will erode when the wind blows, all living things decay, and rocks erode. Statistically it's more probable for something to move from a low entropy state to a high entropy state.  It's possible for the wind to blow over a pile of sand and turn it into a sandcastle, but it would be extremly rare (so rare that it might not happen for the entire lifespan of the universe).  It's more probable that the wind will blow over a sandcastle and turn it into a pile of sand.  Now let's move on from physics and discuss what this means for information theory and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Entropy (Mathematics/Probability Theory Domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"Images/entropy_balls_1.png\" style=\"width:60%\" />\n",
    "</figure> \n",
    "\n",
    "\n",
    "Again let's start with an example, three buckets with balls of different colors.  This is also not an original idea, but put forth by [Luis Serrano](https://www.linkedin.com/in/luisgserrano/). Based on the information provided in the first section of this paper, you may intuitively know which bucket has low, medium, and high entropy.  In fact, if you guessed the buckets have low, medium, and high entropy respectively then you're correct.  Instead of using intuition to rank the buckets with respect to entropy, you may ask the question, \"How many ways can the balls be arranged in each bucket?\"  Let's take a look at the first bucket in the figure below.  The 4 blue balls can only be arranged in one way, if there's no other distiguishing properties other than their color.  The balls in the second bucket can be arranged in 4 different ways and the balls in the last bucket can be arranged in 6 different ways.  To get a better intuition, think about the sandcastle example above and how it relates to this example.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"Images/entropy_balls_2.png\" style=\"width:60%\" />\n",
    "</figure> \n",
    "\n",
    "There's a more precise method to calculate the entropy based on information.  Let's say a random ball is picked from the first bucket, what do we know about the ball?  We know it will be a blue ball because all of them are blue.  That means we have \"high knowledge\" about the system and we can make predictions. Now let's pick a random ball from the second bucket.  We know there's a high probability the ball will be blue and a low probability the ball will be orange, so if we bet on blue we will be correct most of the time.  This means we have \"medium knowledge\" about the system.  Now let's pick a random ball from the last bucket.  We know there's an equal probability the ball will be orange or blue.  This means we have \"low knowledge\" about the system.  As you can see from the figure below, knowledge and entropy are opposite.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"Images/entropy_balls_3.png\" style=\"width:60%\" />\n",
    "</figure> \n",
    "\n",
    "Let's calculate the probabilities for winning this guessing game.  The point of the game is to guess what color ball is going to be picked from each bucket.  For example, if we're going to play this game with the middle bucket, the game will be played as follows: \n",
    "\n",
    "1. Guess a color, blue or orange\n",
    "2. Pick a ball from the middle bucket\n",
    "3. Verify whether the correct color was selected\n",
    "4. Put the ball back into the same bucket\n",
    "\n",
    "A very important rule of the game is to put the ball back before picking up another one. This ensure each pick is an independent event in probability theory.  Two events are independent if the occurrence of one does not affect the probability of occurrence of the other (equivalently, does not affect the odds).  Probability theory and the indepenent event concepts are out of scope for this article, but more can be found in another article [Independence (probability theory)](https://en.wikipedia.org/wiki/Independence_(probability_theory)).  Let's ask the question \"What's the probability of winning the game for the middle bucket?\".  Additionally, we can ask the same question for the other two buckets as well.  To do this, first we have to calculate the probability of picking each ball and this can be shown in the figure directly below. \n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"Images/entropy_balls_4.png\" style=\"width:60%\" />\n",
    "</figure> \n",
    "\n",
    "Second, let's calculate the probability of winning for each bucket.  Picking each ball is an independent event and in probability theory, the probability of picking the correct ball is calculated by multiplying the probability of picking each ball together as shown in the figure below.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"Images/entropy_prob_winning_summary.png\" style=\"width:60%\" />\n",
    "</figure> \n",
    "\n",
    "The more knowledge we have about each bucket the higher the probability of winning the game, which is quite obvious.  Now that we have the probabilities of winning each bucket, we want to calculate the entropy.  Entropy is the opposite of knowledge.  Therefore we want a low number for the first bucket, a high number for the last bucket, and a number in between the low and high numbers for the second bucket.\n",
    "\n",
    "To calculate the probabilities in the table above, we took the product of each probability.  Imagine if there were not only 4 balls, but 1 million balls.  If a million probabilities between 0 and 1 are multiplied together, the result would be a ridiculously small number.  Very small numbers are hard to work with, so we don't want to take the product.  A sum is better than a product which means we need to find a strategy to turn a product into a sum.  In mathematics, a very simple strategy to turn a product into a sum is to use the logarithm identity.  The logarithm is base 2 because of reasons that will be explain later in this article.\n",
    "\n",
    "$$\\log_2(ab) = \\log_2(a) + \\log_2(b)$$\n",
    "\n",
    "Let's apply this identity to the product of probabilities above.  The probabilities that were calculated above will always be between 0 and 1.  The logarithm function is negative between 0 and 1, therefore to make the number positive let's multiply by $-1$.\n",
    "\n",
    "$$-1(\\log_2(ab)) = -1(\\log_2(a) + \\log_2(b))$$\n",
    "\n",
    "$$-1(\\log_2(1.00 \\cdot 1.00 \\cdot 1.00 \\cdot 1.00)) = -1(\\log_2(1.00) + \\log_2(1.00) + \\log_2(1.00) + \\log_2(1.00)) = 0$$\n",
    "\n",
    "$$-1(\\log_2(0.75 \\cdot 0.75 \\cdot 0.75 \\cdot 0.25)) = -1(\\log_2(0.75) + \\log_2(0.75) + \\log_2(0.75) + \\log_2(0.25)) = 3.245$$\n",
    "\n",
    "$$-1(\\log_2(0.50 \\cdot 0.50 \\cdot 0.50 \\cdot 0.50)) = -1(\\log_2(0.50) + \\log_2(0.50) + \\log_2(0.50) + \\log_2(0.50)) = 4$$\n",
    "\n",
    "Now that we calculated the probabilities, the final step to calculate the entropy is to take the average in order to normalize.\n",
    "\n",
    "$$\\dfrac{1}{4}(-1(\\log_2(1.00 \\cdot 1.00 \\cdot 1.00 \\cdot 1.00))) = \\dfrac{1}{4}(-1(\\log_2(1.00) + \\log_2(1.00) + \\log_2(1.00) + \\log_2(1.00))) = 0$$\n",
    "\n",
    "$$\\dfrac{1}{4}(-1(\\log_2(0.75 \\cdot 0.75 \\cdot 0.75 \\cdot 0.25))) = \\dfrac{1}{4}(-1(\\log_2(0.75) + \\log_2(0.75) + \\log_2(0.75) + \\log_2(0.25))) = 0.81125$$\n",
    "\n",
    "$$\\dfrac{1}{4}(-1(\\log_2(0.50 \\cdot 0.50 \\cdot 0.50 \\cdot 0.50))) = \\dfrac{1}{4}(-1(\\log_2(0.50) + \\log_2(0.50) + \\log_2(0.50) + \\log_2(0.50))) = 1$$\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"Images/entropy_balls_5.png\" style=\"width:60%\" />\n",
    "</figure>\n",
    "\n",
    "We have now completed calculating the entropy of each bucket.  The generic formula is described below.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"Images/entropy_balls_6.png\" style=\"width:60%\" />\n",
    "</figure>\n",
    "\n",
    "$$ Entropy = \\dfrac{-m}{m+n} \\log_2\\left(\\dfrac{m}{m+n}\\right)+\\dfrac{-n}{m+n} \\log_2\\left(\\dfrac{m}{m+n}\\right)$$\n",
    "\n",
    "We have only been dealing with balls of two different colors. Instead of colors let's generically call them classes.  The equation for entropy above can easily be adjusted for problems with more classes.\n",
    "\n",
    "$$ Entropy = -\\sum\\limits^{K}_{k=1}y_{k}\\log_2\\left(\\hat{y}_{k}\\right) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
