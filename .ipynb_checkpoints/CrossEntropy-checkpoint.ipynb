{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Entropy Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Entropy (Physics Domain)\n",
    "Neural Networks come in two flavors, logistical regression and classification.  The cross-entropy loss function is used for classification.  First let's define the word entropy. Entropy is a concept that comes from physics, specifically from the second law of thermodynamics.  Entropy explains why all things decay/die, buildings crumble, mountains erode, and why your bedroom takes work to keep it organized.  More formally, entropy is the measure of disorder in a system.  Let's gain a better intuition with an illustration of a real-world example, sand.  This is not an original idea, but was first put forth by [Brian Cox](https://en.wikipedia.org/wiki/Brian_Cox_%28physicist%29), a professor of particle physics in the School of Physics and Astronomy at the University of Manchester.  A pile of sand is composed of individual sand grains.  The sand grains in a pile of sand can be re-arranged an \"infinite\" number of ways and it will still be a pile of sand.  If a sandcastle is built from the pile of sand, then there's less ways the sand can be re-arranged and have it look like the same sandcastle.  So a pile of sand has more entropy then a sandcastle because it has more disorder.  Another way to say this is, the sand in a pile of sand can be re-arranged in many more ways then the sand in the sandcastle and have it remain the same.  Physics says that all things move from a state of low entropy to a state of high entropy over time, which could be the arrow of time itself.  This is way a sandcastle will erode when the wind blows, all living things decay, and rocks erode. Statistically it's much more probable for something to move from a state of low entropy to a state of high entropy.  It's possible for the wind to blow over a pile of sand and turn it into a sandcastle, but it would be extremly rare (so rare that it might not happen for the entire lifespan of the universe).  It's more probable that the wind will blow over a sandcastle and turn it into a pile of sand.  Now let's move on from physics and discuss what this means for information theory and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Entropy (Mathematics/Probability Theory Domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"Images/entropy_balls_1.png\" style=\"width:60%\" />\n",
    "</figure> \n",
    "\n",
    "\n",
    "Again let's start with an example, three buckets with balls of different colors.  Based on the information provided in the first section of this paper, you may intuitively know which bucket has low, medium, and high entropy.  In fact, if you guessed the buckets have low, medium, and high entropy respectively then you're correct.  Instead of using intuition to rank the buckets with respect to entropy, you may ask the question, \"How many ways can the balls be arranged in each bucket?\"  Let's take a look at the first bucket in the figure below.  The 4 blue balls can only be arranged in one way, if there's no other distiguishing properties other than their color.  The balls in the second bucket can be arranged in 4 different ways and the balls in the last bucket can be arranged in 6 different ways.  To get a better intuition, think about the sandcastle example above and how it relates to this example.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"Images/entropy_balls_2.png\" style=\"width:60%\" />\n",
    "</figure> \n",
    "\n",
    "There's a more precise method to calculate the entropy based on information.  Let's say a random ball is picked from the first bucket, what do we know about the ball?  We know it will be a blue ball because all of them are blue.  That means we have \"high knowledge\" about the system and we can make predictions. Now let's pick a random ball from the second bucket.  We know there's a high probability the ball will be blue and a low probability the ball will be orange, so if we bet on blue we will be correct most of the time.  This means we have \"medium knowledge\" about the system.  Now let's pick a random ball from the last bucket.  We know there's an equal probability the ball will be orange or blue.  This means we have \"low knowledge\" about the system.  As you can see from the figure below, knowledge and entropy are opposite.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"Images/entropy_balls_3.png\" style=\"width:60%\" />\n",
    "</figure> \n",
    "\n",
    "Let's calculate some probabilities for winning this game.  The figure below shows the probability of selecting each of the balls.  Selecting the balls are independent events and in probability theory, the probability of all the events happeing is calculated by multiplying all the prodcuts together. To determine the probability of winning the gamew\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"Images/entropy_balls_4.png\" style=\"width:60%\" />\n",
    "</figure> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
