{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Entropy Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Entropy (Physics Domain)\n",
    "Neural Networks come in two flavors, logistical regression and classification.  The cross-entropy loss function is used for classification.  First let's define the word entropy. Entropy is a concept that comes from physics, specifically from the second law of thermodynamics.  Entropy explains why all things decay/die, buildings crumble, mountains erode, and why your bedroom takes work to keep it organized.  More formally, entropy is the measure of disorder in a system.  Let's gain a better intuition with an illustration of a real-world example, sand.  This is not an original idea, but was first put forth by [Brian Cox](https://en.wikipedia.org/wiki/Brian_Cox_%28physicist%29), a professor of particle physics in the School of Physics and Astronomy at the University of Manchester.  A pile of sand is composed of individual sand grains.  The sand grains in a pile of sand can be re-arranged an \"infinite\" number of ways and it will still be a pile of sand.  If a sandcastle is built from the pile of sand, then there's less ways the sand can be re-arranged and have it look like the same sandcastle.  So a pile of sand has more entropy then a sandcastle because it has more disorder.  Another way to say this is, the sand in a pile of sand can be re-arranged in many more ways then the sand in the sandcastle and have it remain the same.  Physics says that all things move from a state of low entropy to a state of high entropy over time, which some physicists think is the arrow of time itself.  This is way a sandcastle will erode when the wind blows, all living things decay, and rocks erode. Statistically it's more probable for something to move from a state of low entropy to a state of high entropy.  It's possible for the wind to blow over a pile of sand and turn it into a sandcastle, but it would be extremly rare (so rare that it might not happen for the entire lifespan of the universe).  It's more probable that the wind will blow over a sandcastle and turn it into a pile of sand.  Now let's move on from physics and discuss what this means for information theory and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Entropy (Mathematics/Probability Theory Domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"Images/entropy_balls_1.png\" style=\"width:60%\" />\n",
    "</figure> \n",
    "\n",
    "\n",
    "Again let's start with an example, three buckets with balls of different colors.  This is also not an original idea, but put forth by [Luis Serrano](https://www.linkedin.com/in/luisgserrano/). Based on the information provided in the first section of this paper, you may intuitively know which bucket has low, medium, and high entropy.  In fact, if you guessed the buckets have low, medium, and high entropy respectively then you're correct.  Instead of using intuition to rank the buckets with respect to entropy, you may ask the question, \"How many ways can the balls be arranged in each bucket?\"  Let's take a look at the first bucket in the figure below.  The 4 blue balls can only be arranged in one way, if there's no other distiguishing properties other than their color.  The balls in the second bucket can be arranged in 4 different ways and the balls in the last bucket can be arranged in 6 different ways.  To get a better intuition, think about the sandcastle example above and how it relates to this example.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"Images/entropy_balls_2.png\" style=\"width:60%\" />\n",
    "</figure> \n",
    "\n",
    "There's a more precise method to calculate the entropy based on information.  Let's say a random ball is picked from the first bucket, what do we know about the ball?  We know it will be a blue ball because all of them are blue.  That means we have \"high knowledge\" about the system and we can make predictions. Now let's pick a random ball from the second bucket.  We know there's a high probability the ball will be blue and a low probability the ball will be orange, so if we bet on blue we will be correct most of the time.  This means we have \"medium knowledge\" about the system.  Now let's pick a random ball from the last bucket.  We know there's an equal probability the ball will be orange or blue.  This means we have \"low knowledge\" about the system.  As you can see from the figure below, knowledge and entropy are opposite.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"Images/entropy_balls_3.png\" style=\"width:60%\" />\n",
    "</figure> \n",
    "\n",
    "Let's calculate the probabilities for winning this guessing game.  The point of the game is to guess what color ball is going to be picked from each bucket.  For example, if we're going to play this game with the middle bucket, the game will be played as follows: \n",
    "\n",
    "1. Guess a color, blue or orange\n",
    "2. Pick a ball from the middle bucket\n",
    "3. Verify whether the correct color was selected\n",
    "4. Put the ball back into the same bucket\n",
    "\n",
    "A very important rule of the game is to put the ball back before picking up another one. This ensure each pick is an independent event in probability theory.  Two events are independent if the occurrence of one does not affect the probability of occurrence of the other (equivalently, does not affect the odds).  Probability theory and the indepenent event concepts are out of scope for this article, but more can be found in another article [Independence (probability theory)](https://en.wikipedia.org/wiki/Independence_(probability_theory)).  Let's ask the question \"What's the probability of winning the game for the middle bucket?\".  Additionally, we can ask the same question for the other two buckets as well.  To do this, first we have to calculate the probability of picking each ball and this can be shown in the figure directly below. \n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"Images/entropy_balls_4.png\" style=\"width:60%\" />\n",
    "</figure> \n",
    "\n",
    "Second, let's calculate the probability of winning for each bucket.  Picking each ball is an independent event and in probability theory, the probability of picking the correct ball is calculated by multiplying the probability of picking each ball together as shown in the figure below.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"Images/entropy_prob_winning_summary.png\" style=\"width:60%\" />\n",
    "</figure> \n",
    "\n",
    "The more knowledge we have about each bucket the higher the probability of winning the game, which is quite obvious.  Now that we have the probabilities of winning each bucket, we want to calculate the entropy.  Entropy is the opposite of knowledge.  Therefore we want a low number for the first bucket, a high number for the last bucket, and a number in between the low and high numbers for the second bucket.\n",
    "\n",
    "To calculate the probabilities in the table above, we took the product of each probability.  Imagine if there were not only 4 balls, but 1 million balls.  If a million probabilities between 0 and 1 are multiplied together, the result would be a ridiculously small number.  Very small numbers are hard to work with, so we don't want to take the product.  A sum is better than a product which means we need to find a strategy to turn a product into a sum.  In mathematics, a very simple strategy to turn a product into a sum is to use the logarithm identity.\n",
    "\n",
    "$$\\log_2(ab) = \\log_2(a) + \\log_2(b)$$\n",
    "\n",
    "Let's apply this identity to the product of probabilities above.  The probabilities that were calculated above will always be between 0 and 1.  The logarithm function is negative between 0 and 1, therefore to make the number positive let's multiply by $-1$.\n",
    "\n",
    "$$-1(\\log_2(ab)) = -1(\\log_2(a) + \\log_2(b))$$\n",
    "\n",
    "$$-1(\\log_2(1.00 \\cdot 1.00 \\cdot 1.00 \\cdot 1.00)) = -1(\\log_2(1.00) + \\log_2(1.00) + \\log_2(1.00) + \\log_2(1.00)) = 0$$\n",
    "\n",
    "$$-1(\\log_2(0.75 \\cdot 0.75 \\cdot 0.75 \\cdot 0.25)) = -1(\\log_2(0.75) + \\log_2(0.75) + \\log_2(0.75) + \\log_2(0.25)) = 3.245$$\n",
    "\n",
    "$$-1(\\log_2(0.50 \\cdot 0.50 \\cdot 0.50 \\cdot 0.50)) = -1(\\log_2(0.50) + \\log_2(0.50) + \\log_2(0.50) + \\log_2(0.50)) = 4$$\n",
    "\n",
    "Now that we calculated the probabilities, the final step to calculate the entropy is to take the average in order to normalize.\n",
    "\n",
    "$$\\dfrac{1}{4}(-1(\\log_2(1.00 \\cdot 1.00 \\cdot 1.00 \\cdot 1.00))) = \\dfrac{1}{4}(-1(\\log_2(1.00) + \\log_2(1.00) + \\log_2(1.00) + \\log_2(1.00))) = 0$$\n",
    "\n",
    "$$\\dfrac{1}{4}(-1(\\log_2(0.75 \\cdot 0.75 \\cdot 0.75 \\cdot 0.25))) = \\dfrac{1}{4}(-1(\\log_2(0.75) + \\log_2(0.75) + \\log_2(0.75) + \\log_2(0.25))) = 0.81125$$\n",
    "\n",
    "$$\\dfrac{1}{4}(-1(\\log_2(0.50 \\cdot 0.50 \\cdot 0.50 \\cdot 0.50))) = \\dfrac{1}{4}(-1(\\log_2(0.50) + \\log_2(0.50) + \\log_2(0.50) + \\log_2(0.50))) = 1$$\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"Images/entropy_balls_5.png\" style=\"width:60%\" />\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
